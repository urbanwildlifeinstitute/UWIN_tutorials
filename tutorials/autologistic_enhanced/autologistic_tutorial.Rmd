---
title: "Autologistic occupancy modeling"
output:
  html_document:
    toc: false
  #md_document:
   # toc: false
date: "`r Sys.Date()`"
knit: (function(inputFile,encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_format = "all") } )
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
library(autoOcc)
library(dplyr)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


NOTE: For this code to work as intended, your working directory should be set to
`"./tutorials/autologistic_enhanced"` where the `.` represents
the location of the `UWIN_tutorials.Rproj` file on your computer. If you 
double-click on `UWIN_tutorials.Rproj` on your computer a new window of
Rstudio will open up and you can enter `setwd("./tutorials/autologistic_enhanced")`
into your R console.


### Tutorial Aims:

#### <a href="#occupancy"> 1. What are Autologistic occupancy models?</a>

#### <a href="#packages"> 2. Load necessary packages</a>

#### <a href="#formatting"> 3. Formatting data</a>

#### <a href="#models"> 4. Fitting models</a>

#### <a href="#plots"> 5. Predicting & plotting model outputs</a>



<a name="occupancy"></a>

## 1. What are Autologistic occupancy models?


Though static occupancy models can be a useful tool to study a species distribution at a single point in time they are limited to a 'static' system, meaning we cannot account for temporal changes in the environment or species occupancy. By considering a dynamic system (i.e., one that can change over time), we can account for things like changing environmental conditions (such as climate or fire), impacts of management interventions, or variations in sampling methodologies. Often, scientists are interested in modeling temporal dynamics or how a species' habitat use changes across time. There are [a variety of ways we can model](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12100) such a dynamic system. Some common ways to account for temporal changes include dynamic occupancy models, multi-state occupancy models, or with random effects. However, all these methods require large amounts of data to estimate all affiliated parameters. An alternative to these methods is to add a temporal autologistic parameter to the model so that the occupancy status of a species at one time point depends in part on the occupancy status in the previous timestep. Unlike the other methods listed above, autologistic occupancy models only introduce one new parameter. To get into the nitty gritty of the equations, please review [this blog post](https://masonfidino.com/autologistic_occupancy_model/) (also listed in references above) or check out the help file for fitting autologistic occupancy models (
i.e., `?autoOcc::auto_occ`).

<a name="packages"></a>

## 2. Load necessary packages

We are going to need three packages for this tutorial. You should already
have the `autoOcc` R package installed. If not, then install the `devtools`
package so that you can download the package from GitHub. The readme file
for `autoOcc` has those details [if you need to install that package](https://github.com/mfidino/autoOcc). Aside from `autoOcc` we are
going to need two packages from the `tidyverse`: `dplyr` and `ggplot2`. If
you do not have `dplyr` or `ggplot2` then the code below will download them
for you.

```{r packages, eval = FALSE}
# To conduct the analysis
library(autoOcc)

# To summarise data
if("dplyr" %in% installed.packages()){
  library(dplyr)
} else {
  install.packages("dplyr")
  library(dplyr)
}

# To plot results
library(ggplot2)
if("ggplot2" %in% installed.packages()){
  library(ggplot2)
} else {
  install.packages("ggplot2")
  library(ggplot2)
}

```

<a name="formatting"></a>

## 3. Formatting data for an Autologistic model
For this example, we are going to be modeling coyote (*Canis latrans*) occupancy from
data we collected throughout Chicago, Illinois. The `autoOcc` package contains the
utility function `format_y()`, which can be used to prepare the data for analsysis.
However, to ensure that `format_y()` works correctly, the data you supplied to it
must be properly formatted. Specifically, we need a `data.frame` that contains:

 - A column that provides the name of the site that data point is associated with
 - A column that provides the primary sampling period that data point is associated with
 - Columns that are named in a similar way that contain a species detection history.
 
 
 As a small example, this would be an appropriately formatted `data.frame`

| season  | site | occ_1 | occ_2 | occ_3 |
|---------|------|------:|-------:|------:|
| Season1 | A    | NA    | 0      | 1     |
| Season1 | B    | NA    | 0      | 0     |
| Season1 | C    | 0     | 1      | 1     |
| Season2 | A    | 1     | 0      | 0     |
| Season2 | B    | NA    | NA     | NA    |
| Season2 | C    | 0     | 0      | 1     |


Some important aspects of this data.frame are:

- It is sorted by season, then site, and seasons are sorted temporally from the
first to the last.

- The columns with the detection history all start the same. The `format_y()` 
function uses regular expressions to collect these columns. Thus, this is the
easiest way to ensure you collect all of them.

- Detection histories can either have a `1` (species detected), `0` (species not
detected), or `NA` (sampling did not occur on that occasion).

- It is okay if not every site is represented within each season. However, site
names for a given sampling location cannot change across seasons. For example, if 
`HUP2` was the name of a sampling location then that site must be named `HUP2`
across all seasons.  `HUP3` would be treated as a new site.

Okay, now let's load in the coyote detection / non-detection data and
take a quick look at it with `dplyr::glimpse()`. The coyote data is
within the data sub-folder and it is titled `chicago_coyote.csv.`

```{r read_in_coyote}
# read in the coyote detection data
coyote <- read.csv(
  "./data/chicago_coyote.csv"
)


# check out the structure of the data
dplyr::glimpse(coyote)

```

We can see here that we have columns for `Season`, `Site`, and the detection / 
non-detection data (i.e., `Week_1` through `Week_9`).

### Challenge 1. Use `format_y()`

Check out the help file for `autoOcc::format_y()` to see what the arguments
are for this function. After reading through them, apply `format_y()` to the 
`coyote` data.frame. It is okay to overwrite the `coyote` object with the 
output from `format_y()`

```{r challenge_1}

# Solution is below if you need it to compare to what you coded up!

```


<br>

<details closed>
<summary>
Solution</a>
</summary>
```{r, format_y_coyote}
  coyote <- autoOcc::format_y(
    x = coyote, # coyote data.frame
    site_column = "Site", # Name of site column in coyote
    time_column = "Season", # Name of season column in coyote 
    history_columns = "Week" # what the detection history columns start with.
  ) 
```
</details>


<br>

The output of `format_y` is a three-dimensional array. The dimensions are
site by primary sampling period by secondary sampling period (i.e., weeks
of sampling within each season). If we wanted to see all the detection data
for site 15 we can use subset notation, but don't forget this has three dimensions!

```{r site_15_coyote}

coyote[15,,]

```

One last thing you may want to ensure is that all of your sites actually
have data. Sites with zero data across all seasons will be dropped from 
the analysis in autoOcc, but it's better to just ensure you remove them
first. Since this is an array the easiest way to do this is with the `apply()`
function. Since we just want to sum across all the
data for each site, the dimension (i.e., MARGIN)
we want to apply our function to is the first one. 

```{r check_sites}

nweeks_sampled <- apply(
  X = coyote,
  MARGIN = 1,
  function(x){
    sum(!is.na(x))
  }
)

# Check out a little histogram of this
hist(
  nweeks_sampled,
  main = "Number of weeks sampled per site",
  xlab = "Weeks sampled"
)

```

<br>

Now that we have the detection data formatted. It is time to read in the 
covariate data. The `autoOcc` package can handle covariates that vary spatially,
temporally, or spatiotemporally. For this tutorial we are going to have two 
spatial covaraites and one spatiotemporal covariate. Thes are:

- An urban intensity metric we will generate with a principal components analysis 
- The distance of each site to a stream or river
- The average weekly temperature of each week of sampling

One thing that is very important to remember is that unlike the detection data,
the covariates CANNOT have missing data (i.e., `NA` values).


To read in the spatial covariates, copy this code here:

```{r read_in_covars}

covs <- read.csv(
  "./data/coyote_covariates.csv"
)

dplyr::glimpse(covs)

```

<br>

This dataset has 100 rows (we have 100 sites worth of data) and five columns:

- `Site`: The site the covariates are associated to. This is ordered exactly
the same is the `coyote` data, and site names are identical among the two
datasets.
- `tree`: The proportion of tree cover within 1000m of a site. To be used for
urban intensity PCA.
- `imperv`: The proportion of impervious cover within 1000m of a site. To be used
for urban intensity PCA.
- `housing_density`: The number of houses per square km within 1000m of a site. To
be used for urban intensity PCA.
- `dist_stream`: The distance of each site to a stream or river in km.


To generate the urban intensity term, we are going to use `dplyr` to select
only the columns we need and then use the `prcomp()` function in base `R`.

```{r make_pca}
# make an urbanization metric with tree cover (tree), impervious
#  cover (imperv), and housing density (housing_density)

urb_pca <- covs %>% 
  dplyr::select(tree, imperv, housing_density) %>% 
  prcomp(., center = TRUE, scale = TRUE)

# The first thing we want to see is how much variance each
#  component explains
summary(urb_pca)


```

The first principal component (PC1) explains about 70% of the variation in the data,
so we will go ahead and use only that one. We can interpret this term by looking
at the loadings associated to `urb_pca`. We are going to round them as well as
we don't need to see that many significant digits.

```{r check_loadings}
round(
  urb_pca$rotation,
  2
)
```

### Challenge 2: Interpret PC1

The first principal component can be accessed via `urb_pca$x[,1]`. However,
we don't need to look at those to be able to interpret this, we only need
the loadings above. What do you think they mean?


<br>

<details closed>
<summary>
Solution</a>
</summary>
 For `PC1` we see that `imperv` and `housing_density` are both positive, while
  tree is negative. This means that negative values of the urb_pca are
  locations high in tree cover whereas positive values of the urb_pca
  are locations high in impervious cover and housing density. So negative
  is more forested and positive is more urban.
</details>

<br>

The second covariate we have is the distance of each site to a stream
or river. We included this one as riparian areas are often
used a corridors for mammals. However, before we include it
into our covariate data.frame for analysis we need to scale it (we set
scale = `TRUE` when making the principal component term). Why do you scale 
covariates for an analysis? It makes it more likely for the statistical 
algorithms used in `autoOcc` to converge. Scaling also makes it so you
can compare the magnitude of slope terms in your model. As our occupancy
covariates do not have any spatio-temporal data, we can assemble our
covariates in a `data.frame`.



```{r occ_covs}

occ_covs <- data.frame(
  urb = urb_pca$x[,1], # the PC1 term.
  dist_stream = as.numeric(
    scale(
      covs$dist_stream
    )
  )
)
```

In addition to these covariate data, we also have weekly temperature
data to include as a detection covariate. Since Chicago experiences
four distinct seasons, coyote may alter their behavior when it's either
hot (the summer) or when its cold (the winter). This is also 
to show how to include survey-specific covariates on detection, which
is something `autoOcc` can easily handle.


```{r read_in_temp}
week_temp <- read.csv(
  "./data/weekly_temp_data.csv"
)
dplyr::glimpse(week_temp)

```

You want to have the `data.frame` set up so all the data
for each site is grouped together, and each grouping is arranged temporally from
the beginning to the end of the study. Thus, this `data.frame` is set up to vary by week, season, and then site. 

Before we arrange the data, let's query some of the dimensions we are going
to need to set it up.

```{r query_dimensions}
# right now the data are set up so it varies by week, season, and then site. 
nweek <- dplyr::n_distinct(week_temp$Week)
nseason <- dplyr::n_distinct(week_temp$Season)
nsite <- dplyr::n_distinct(week_temp$Site)
```

To set up this covariate we are going to need to create a matrix with a 
number of rows equal to `nsite` and a number of columns equal to `nweek * nseason`
the first `nweek` columns will be for the first season, then the next `nweek`
are for season 2, etc., etc. So each row is going to hold all the temperature
data for one site. Because of the way the ordering in week_temp (week,
season, then site) we can just input the covariate and fill the `matrix`
in the correct way by setting the `byrow` argument to `TRUE`.


## Challenge 3. Make the temperature matrix.

The paragraph right above challenge three has all the info you would need
to correctly generate the `matrix` in `R`. Give it a go!

```{r challenge_3}

# Solution is below

```

<details closed>
<summary>
Solution</a>
</summary>
```{r, make_temp_matrix}
temp_matrix <- matrix(
  week_temp$temp,
  ncol = nweek * nseason,
  nrow = nsite,
  byrow = TRUE
)
```
</details>

<br>


Now, all columns of this matrix are associated to the same covariate, and so
to scale this covariate we need to divide each column by the global mean
and divide by the global standard devation. Since this is a matrix this is a bit easier to just do by hand instead of using the `scale` function.

```{r scale_temp}
temp_matrix <- (
  temp_matrix - mean(temp_matrix)
) / sd(temp_matrix)

```

Finally, we are also going to include urban intensity as a detection covariate.
We are doing this because local abundance can influence detection probability,
and coyote abundance likely covaries with urban intensity. Unlike the occupancy
covariates, we will have one spatio-temporal covariate and one spatial covariate.
Thus, we need to use a named `list` object for the detection covariates instead
of a `data.frame`. If your detection covariates are only spatial in future analyses
you can just use a `data.frame`.


```{r make_det_covs}
det_covs <- list(
  temp = temp_matrix,
  urb = occ_covs$urb
)

```


<a name="models"></a>

## 4. Fitting models

The `autoOcc` package was based on `unmarked`, so if you have any familiarity
with `unmarked` you will find that the equation syntax is identically. If you
are unfamiliar, for this model you need to specify two linear predictors to
fit the model (detection then occupancy). Spefically, the `auto_occ()` function
uses a double-right hand side formula, which means you do not need to specify
what the response variable in the formula itself. For example, the syntax for
an intercept only model would be `~1~1`.




For this workshop we are going to fit five models. There are certainly others
that could be fitted with the supplied covariates, but this is a decent start.
You will notice that all of our models include the same linear predictors for
detection, included both linear and quadratic responses to urban intensity
and weekly temperature. The main reason for this is just because we are not
too interested in trying to determine the most parsimonious detection model and
the AIC of these models can be quite sensitive to changes in detection covariates.

These models we will fit include:

- Occupancy does not covary with any covariates. The null. (`~1`)
- Occupancy is associated with urban intensity (`~urb`)
- Occupancy is associated with distance to a stream or river (`~dist_stream`)
- Occupancy is associated with both spatial covariates (`urb + dist_stream`)
- Occupancy is associated with both spatial covariates, and there is an
interaction betwen the two (`urb * dist_stream`)


To fit an autologistic occupancy model you use the `autoOcc::auto_occ()` function.
Go ahead and copy / paste this code into your script and run fit them all.
Note that if you changed the names of any of the objects above you will also
need to modify them here. Finally, note that I am using 85% confidence intervals
here instead of the standard 95% most people use. Why? [It aligns more closely
with the use of AIC](https://royalsocietypublishing.org/doi/full/10.1098/rspb.2023.1261). 

But before we fit all the models, let's fit the simplest and look at a summary
of the output.

```{r fit_null}

# the null model
null <- auto_occ(
  ~temp + I(temp^2) + urb + I(urb^2)
  ~1, # detection linear predictor first, then occupancy
  y = coyote,
  occ_covs = occ_covs,
  det_covs = det_covs,
  level = 0.85 # Using 85% CI because it aligns more with the use of AIC
)

(my_summary <- summary(null))


```

You can see above that we have summaries for both levels of the model here, and
they are labeled. Furthermore, even though we specified no covariates for 
occupancy, it has two parameters associated to it: the intercept and this theta
term. That second one is the temporal autologistic term which is always included
in this model. Specifically, this model estimates that the model intercept
is `-0.839` and the theta term is `2.245`, which is STRONGLY positive.

## Challenge 4. Have these numbers make sense.

These estimates are on the logit-scale. Calculate three things with these values

- Coyote occupancy if they were not present in the previous timestep
- Coyote occupancy if they were present in the previous timestep
- The expected occupancy over time

As a hint, the help file for the `auto_occ` function may have some information
that is relevant.

<details closed>
<summary>
Solution</a>
</summary>
```{r, occ_calc}
# If you want to hard code it
b0 <- -0.839
theta <- 2.245

# occupancy if not present at t-1
coy_occ <- plogis(b0)

# Occupancy if present at t
coy_occ_theta <- plogis(b0 + theta)

# Expected occupancy
coy_ex_occ <- coy_occ / (coy_occ + (1 - coy_occ_theta))

# Or, if you wanted to query the parameters from the summary output, which
#  is an s4 class in R.
psi_parms <- my_summary@psi

b0 <- my_summary@psi$Est[1]
theta <- my_summary@psi$Est[2]


# occupancy if not present at t-1
coy_occ <- plogis(b0)

# Occupancy if present at t
coy_occ_theta <- plogis(b0 + theta)

# Expected occupancy
coy_ex_occ <- coy_occ / (coy_occ + (1 - coy_occ_theta))


```
</details>

<br>


From here, let's fit the remaining models. Go ahead and copy this code over
to your R script and run them.

```{r fit_models}



# urban intensity model
urb <- auto_occ(
  ~temp + I(temp^2) + urb + I(urb^2)
  ~ urb,
  y = coyote,
  occ_covs = occ_covs,
  det_covs = det_covs,
  level = 0.85
)

# stream model
stream <- auto_occ(
  ~temp + I(temp^2) + urb + I(urb^2)
  ~ dist_stream,
  y = coyote,
  occ_covs = occ_covs,
  det_covs = det_covs,
  level = 0.85
)

# urban + stream
urb_stream <- auto_occ(
  ~temp + I(temp^2) + urb + I(urb^2)
  ~ urb + dist_stream,
  y = coyote,
  occ_covs = occ_covs,
  det_covs = det_covs,
  level = 0.85
)

# urban * stream (interaction model)
urb_stream_inxs <- auto_occ(
  ~temp + I(temp^2) + urb + I(urb^2)
  ~ urb*dist_stream,
  y = coyote,
  occ_covs = occ_covs,
  det_covs = det_covs,
  level = 0.85
)

```

## Compare models ####

After fitting our models, it is time to compare their relative fit using AIC.
As a reminder, information theoretic approaches like AIC do not ensure you
top model is good, it just has a better relative fit than the other models
in your model set. If you are interested in further reading about AIC, I 
would greatly encourage looking up [Arnold (2010)](https://wildlife.onlinelibrary.wiley.com/doi/abs/10.1111/j.1937-2817.2010.tb01236.x) and [Sutherland et al. (2024)](https://royalsocietypublishing.org/doi/full/10.1098/rspb.2023.1261).

To compare our models, we just need to use the `compare_models()` function
in `autoOcc`. This function requires a list object that contains all the models
in our model set. If the list is named, then those names will transfer over
to the AIC table.

```{r compare_models}
coyote_models <- list(
  null = null,
  urb = urb,
  stream = stream,
  urb_stream = urb_stream,
  urb_stream_inxs = urb_stream_inxs
)

# digits = 2 to set number of significant digits
coyote_aic <- compare_models(
  model_list = coyote_models,
  digits = 2
)

# check out the results
coyote_aic

```
Using 2 delta AIC is a cutoff, it appears that there is essentially one
competitive model. If we wanted to entertain the `urb_stream` model, note that 
it is exactly 2 AIC from the best-fit model. As AIC goes up by 2 for each 
parameter in the model, the inclusion of the stream variable actually did
nothing to further explain the data. Thus, the stream covariate is likely
an uninformative parameter, meaning we may as well focus on just the `urb`
model. Let's take a peek.

```{r urb_summary}

best_model <- summary(urb)

best_model
```

## Challenge 5 Interpret the model output:



1. As urban intensity goes up, what happens with coyote occupancy?
2. What had a greater effect on detection probability? Temperature or
urban intensity?
3. Given the non-linear terms in the detection model, what do you think
the relationship between temperature and detection looks like? What about
with urban intensity and detection?



<details closed>
<summary>
Solution</a>
</summary>

1. As urban intensity goes up, coyote occupancy goes down because the slope
term is negative (-0.248).

2. Urban intensity likely had a greater influence on detection probability
because the slope term `rho-urb` is more negative than `rho - temp`. However,
there is some uncertainty here as the quadratic terms differ a lot. Chances
are the only way we'll really be able to figure this out is to plot the 
relationships.

3. Temperature has a negative linear term and a positive quadratic term. Thus,
this relationship is negative but convex (goes up at the very low and
high temperatures). Urban intensity has a negative linear term and a negative
quadratic term. Thus, this relationship is negative but concave (slightly
higher at intermediate levels of urban intensity).

</details>

<br>




```{r plot_stuff}
range(week_temp$temp)

nice_temps <- seq(-15, 25, length.out = 300)
temp_pred_df <- data.frame(
  temp = nice_temps,
  urb = 0
)

# scale the temperature data like we did for our model
temp_pred_df$temp <- (temp_pred_df$temp - mean(week_temp$temp)) / sd(week_temp$temp)



det_temp_preds <- predict(
  object = urb,
  type = "rho",
  newdata = temp_pred_df,
  level = 0.85,
  seed = 154
)

det_temp_plot <- data.frame(
  temp = nice_temps,
  det_temp_preds
)

# An expression to get the degrees symbol
xlab_expression <- expression(
  Temperature~(degree*C)
)
# and plot this out with ggplot
ggplot(det_temp_plot, aes(x = temp, y = estimate)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "#72AD8F", alpha = 0.5) +
  geom_path(linewidth = 1) + # adds line
  labs(x = xlab_expression, y = "Detection probability") +
  scale_x_continuous(limits = range(nice_temps)) +
  ylim(0,1)+
  theme_classic()+ # drops gray background and grid
  theme(plot.title=element_text(size = 16, hjust=0.5), # centers titles
        axis.text.x = element_text(size = 12, color = "black"),    
        axis.text.y = element_text(size = 12, color = "black"),
        axis.title = element_text(size = 18)) 



```
## Plotting out the results
 
To do this, we need to generate a new data.frame with all the
associated covariates tied to a given level of the model (i.e.,
occupancy or detection). However, we scaled our covariates,
and to generate our predictions they need to be scaled
EXACTLY as the data we supplied to the model. In our
case, this is pretty easy. The urban intensity covariate
is already kind of abstract, so we just need to generate
a sequence of values from around the same extent as the
data we supplied. Following this, `autoOcc` has it's own
predict function, and it's help file can be looked up with
`?predict.auto_occ_fit`.

One unique aspect with making these model predictions is that this function
derives the expected occupancy (see `?auto_occ` for more details). It does
so through some simulations, and so if you want to return the exact same
values each time it is a good idea to set a seed. The output of `predict` returns
a data.frame with three columns: `estimate`, `lower`, and `upper` which 
respectively represent the mean estimate and the lower and upper confidence
intervals.
  
```{r plot_psi_urb}

# Look up the range 
range(occ_covs$urb)

# great data.frame for new predictions
occ_pred_df <- data.frame(
  urb = seq(-3, 4.5, length.out = 300)
)

# for help, see ?autoOcc::predict.auto_occ_fit
occ_preds <- predict(
  object = urb,
  type = "psi",
  newdata = occ_pred_df,
  level = 0.85,
  seed = 153
)

# add the urbanization covariate onto the predicions.
urb_plot <- data.frame(
  urb = occ_pred_df$urb,
  occ_preds
)

# and plot this out with ggplot
ggplot(urb_plot, aes(x = urb, y = estimate)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "#72AD8F", alpha = 0.5) +
  geom_path(linewidth = 1) + # adds line
  labs(x = "Urban Intensity", y = "Occupancy") +
  scale_x_continuous(limits = c(-3,4.5)) +
  ylim(0,1)+
  theme_classic()+ # drops gray background and grid
  theme(plot.title=element_text(size = 16, hjust=0.5), # centers titles
        axis.text.x = element_text(size = 12, color = "black"),    
        axis.text.y = element_text(size = 12, color = "black"),
        axis.title = element_text(size = 18)) 


```

Plotting out the results for the detection part of the model is just a little
more complex because this level of the model contains two covariates. To plot 
these out we are going to have to assess the marginal effect of each, which means
we will vary one of the covariates while keeping the other at a constant value 
(the mean of that covariate). Because we have centered and scaled our continuous
covaraites, their means are zero (which effectively removes them from the
model prediction).  Let's plot out the temperature results, and then leave
the urban intensity plot as the last challenge.

```{r rho_temperature}

# What is the range of temperatures?
range(week_temp$temp)

# seems like somewhere between -15C and 25C is good for making
#  a 'pretty' x-axis
nice_temps <- seq(-15, 25, length.out = 300)
temp_pred_df <- data.frame(
  temp = nice_temps,
  urb = 0 # don't forget to include urb here!
)

# scale the temperature data like we did for our model
temp_pred_df$temp <- (temp_pred_df$temp - mean(week_temp$temp)) / sd(week_temp$temp)


# Generate predictions, this time switching type to 'rho' because it
#  is for detection probability.
det_temp_preds <- predict(
  object = urb,
  type = "rho",
  newdata = temp_pred_df,
  level = 0.85,
  seed = 154
)

# add temps into the data.frame for plotting purposes
det_temp_plot <- data.frame(
  temp = nice_temps,
  det_temp_preds
)

# An expression to get the degrees symbol for the x-axis
xlab_expression <- expression(
  Temperature~(degree*C)
)
# and plot this out with ggplot
ggplot(det_temp_plot, aes(x = temp, y = estimate)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "#72AD8F", alpha = 0.5) +
  geom_path(linewidth = 1) + # adds line
  labs(x = xlab_expression, y = "Detection probability") +
  scale_x_continuous(limits = range(nice_temps)) +
  ylim(0,1)+
  theme_classic()+ # drops gray background and grid
  theme(plot.title=element_text(size = 16, hjust=0.5), # centers titles
        axis.text.x = element_text(size = 12, color = "black"),    
        axis.text.y = element_text(size = 12, color = "black"),
        axis.title = element_text(size = 18)) 


```
## Challenge 6: Make the urban intensity detection figure

Hint: You should be able to piece together the code to do this from the last two
figures.

<details closed>
<summary>
Solution</a>
</summary>
```{r, rho_urb_plot}

urb_pred_df <- data.frame(
  temp = 0,
  urb = occ_pred_df$urb
)

det_urb_preds <- predict(
  object = urb,
  type = "rho",
  newdata = urb_pred_df,
  level = 0.85,
  seed = 154
)

det_urb_plot <- data.frame(
  urb = urb_pred_df$urb,
  det_urb_preds
)


# and plot this out with ggplot
ggplot(det_urb_plot, aes(x = urb, y = estimate)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "#72AD8F", alpha = 0.5) +
  geom_path(linewidth = 1) + # adds line
  labs(x = "Urban Intensity", y = "Detection probability") +
  scale_x_continuous(limits = range(det_urb_plot$urb)) +
  ylim(0,1)+
  theme_classic()+ # drops gray background and grid
  theme(plot.title=element_text(size = 16, hjust=0.5), # centers titles
        axis.text.x = element_text(size = 12, color = "black"),    
        axis.text.y = element_text(size = 12, color = "black"),
        axis.title = element_text(size = 18)) 
```
</details>

<br>
